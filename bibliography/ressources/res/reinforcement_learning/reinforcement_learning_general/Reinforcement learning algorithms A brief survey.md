from Ashish Kumar Shakya, Gopinatha Pillai, Sohom Chakrabarty

## Resume in few words
Paper dedicated to explore the realm of RL, present the main algorithms, does not deep dive into them.

## Introduction
Reinforcement Learning (RL) is a machine learning paradigm where an agent / multiple agents learn(s) to make sequences of decisions by interacting with an environment, aiming to maximize a reward signal over time. It involves learning from trial and error by receiving feedback in the form of rewards or penalties for its actions based humans/animal learning, aiming to discover the best strategies or policies to achieve long-term goals in uncertain environments.

ðŸŸ¢adaptability compared to other scientific and engineering fields  

RL combines human learning, the combination of [dynamic programming](https://en.wikipedia.org/wiki/Dynamic_programming) and [temporal difference](https://en.wikipedia.org/wiki/Temporal_difference_learning). 

In <u>1911</u> (the paper) or <u>1898 </u>(wiki), the principle of trial and error learning principle  ["Law of Effect"](https://en.wikipedia.org/wiki/Law_of_effect) was theorized by Edward Thorndike : "the essential idea is that behavior can be modified by its consequences". Behaviors that result in satisfying outcomes lead to an increased probability of repeating those behaviors.

In <u>1952</u>, Shannon demonstrated the first successful application of the "Law of Effect"on a maze running mechanical mouse.

In <u>1968</u>, Michie and Chambers introduced GLEE a RL agent and a controller BOXES for the pole balancing task.


